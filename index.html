**Llama-3[8B] Meditron V1.0**
                  The first [*Llama-3*](https://llama.meta.com/llama3/)  fine/instruct-tuning on biomedical data
                                          <small>*by the Meditron Team @EPFL/Yale*</small>

<img src="static/images/Logo.png">


To address the need for accessible technology in resource-limited settings, our team has finetuned the Llama-3-8B model as an open-source solution. This initiative was launched on Friday, April 19th, at 4 AM AoE (Anywhere on Earth). Under the 24-hour window, our team of 10 students achieved SOTA: The power of open-source.


RESULTS
=============================

Using the benchmarking tool <https://github.com/EleutherAI/lm-evaluation-harness> we compared Llama-3[8B] (base) vs. Meditron[7B] vs. Llama-3[8B]-Meditron V1.0 on 3 main benchmarks (MedQA, PubMedQA, MedMCQA)


<!-- | Model             | MMLU-Medical | PubMedQA | MedMCQA | MedQA | MedQA-4-Option | Avg  |
|-------------------|--------------|----------|---------|-------|----------------|------|
| **BioMistral**  | -            | 68.1     | -       | 36.7  | -              | -    |
| **Meditron 1** | -            | 55.8     | 41.0    | -     | 38.1           | -    |
| ...               | ...          | ...      | ...     | ...   | ...            | ...  |
| **Llama-2-7B**      | 56.3         | 61.8     | 54.4    | 44.0  | 49.6           | 53.2 |
| **MEDITRON-7B**     | 55.6         | 74.4     | 59.2    | 47.9  | 52.0           | 57.5 |
| ...               | ...          | ...      | ...     | ...   | ...            | ...  |
| **MEDITRON-70B**    | 77.6         | 81.6     | 66.7    | 70.8  | 75.8           | 74.5 | -->

<img src="results.png" alt="Results">



STAY TUNED for more tuning!
=================================



This is a first iteration...many more versions to come!

<img src="static/images/graph.png">


ABOUT US
=================
**Alexandre Sallinen**<sup>1,2</sup>,  
**Guillaume Boye**<sup>1,2</sup>,  
**Michael Zhang**<sup>1,2</sup>,  
**Maud Dupont-Roc**<sup>1,2</sup>,  
**Bastien Bernath**<sup>1,2</sup>,  
**Etienne Boisson**<sup>1,2</sup>,  
**Xavier Theimer-Lienhard**<sup>1,2</sup>,  
**Hichem Hadhri**<sup>1,2</sup>,  
**Antoine Tran**<sup>1,2</sup>,  
**Martin Jaggi**<sup>2</sup>,  
**Veronique Suttels**<sup>4</sup>,  
**No√©mie Boillat-Blanco**<sup>4</sup>,  
**Kristina Keitel**<sup>5</sup>,  
**Mary-Anne Hartley**<sup>1,2,3</sup>  

**1** LiGHT: Laboratory for Intelligent Global Health and Humanitarian Response Technologies, Yale-EPFL 

**2** EPFL, School of Computer and Communication Sciences, Switzerland.  

**3** Yale University, School of Medicine, Biomedical Informatics and Data Science, USA.  

**4** CHUV, Infectious Diseases Service, Rue du Bugnon 46, Lausanne, Switzerland.  

**5** Inselspital, Department of Pediatrics, Freiburgstrasse 20, Bern, Switzerland. 

<img src="static/images/Affiliations.png">

LiGHT: Laboratory for intelligent Global Health and Humanitarian Response Technologies
  <https://www.yale-light.org/>
MLO: Machine Learning & Optimization Lab
  <https://www.epfl.ch/labs/mlo/>


                                          
<!-- Slider -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script src="../resources/jquery.event.move.js"></script>
<script src="../resources/jquery.twentytwenty.js"></script>
<link href="../resources/offcanvas.css" rel="stylesheet">
<link href="../resources/twentytwenty.css" rel="stylesheet" type="text/css" />
<script>var markdeepOptions = {onLoad: function() {$(".twentytwenty-container").twentytwenty({default_offset_pct: 0.5, move_slider_on_hover: true});} };</script>
<!-- Markdeep: -->
<script src="https://morgan3d.github.io/markdeep/latest/markdeep.min.js?" charset="utf-8"></script>
<script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>
